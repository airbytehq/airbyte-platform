<?xml version="1.0" encoding="UTF-8"?>
<Configuration status="INFO">
    <Properties>
        <Property name="ci-mode">${sys:ciMode:-false}</Property>
        <!--
        date format is datadog friendly so that it can try to detect multilines logs
        https://github.com/DataDog/datadog-agent/blob/a27c16c05da0cf7b09d5a5075ca568fdae1b4ee0/pkg/logs/internal/decoder/auto_multiline_handler.go#L208

        trace_id and span_id are added to the logs so that datadog can correlate logs and traces https://docs.datadoghq.com/tracing/other_telemetry/connect_logs_and_traces/
        https://app.datadoghq.com/logs/pipelines?search=env%3Aci
         -->
        <Property name="pattern-with-trace-id">%d{yyyy-MM-dd HH:mm:ss,SSS}{GMT+0} [dd.trace_id=%X{dd.trace_id} dd.span_id=%X{dd.span_id}] %p %C{1.}(%M):%L %replace{%m}{apikey=[\w\-]*}{apikey=*****}%n</Property>
        <!-- Mask the string apikey=<string> to apikey=***** to prevent secrets leaking. -->
        <Property name="default-pattern">%d{yyyy-MM-dd HH:mm:ss}{GMT+0} %highlight{%p} %C{1.}(%M):%L - %replace{%m}{apikey=[\w\-]*}{apikey=*****}%n</Property>
        <!--Logs the timestamp and log_source/application name in the beginning of the line if it exists with a > separator, and then always the rest of the line.-->
        <Property name="simple-pattern">%d{yyyy-MM-dd HH:mm:ss}{GMT+0}%replace{ %X{log_source}}{^ -}{} > %replace{%m}{apikey=[\w\-]*}{apikey=*****}%n</Property>

        <!-- Always log INFO by default. -->
        <Property name="log-level">${sys:LOG_LEVEL:-${env:LOG_LEVEL:-INFO}}</Property>

        <Property name="route-ttl">${env:LOG_IDLE_ROUTE_TTL:-15}</Property>

        <!-- Note that logging to S3 will leverage the DefaultAWSCredentialsProviderChain for auth. -->
        <Property name="azure-blob-container">${sys:STORAGE_BUCKET_LOG:-${env:STORAGE_BUCKET_LOG:-}}</Property>
        <Property name="azure-storage-connection-string">${sys:AZURE_STORAGE_CONNECTION_STRING:-${env:AZURE_STORAGE_CONNECTION_STRING:-}}</Property>
    </Properties>

    <Appenders>
        <Console name="Default" target="SYSTEM_OUT">
            <!-- Existing ContextMapFilter behavior when ciMode is not true -->
            <ContextMapFilter onMatch="DENY" onMismatch="ACCEPT">
                <KeyValuePair key="simple" value="true"/>
            </ContextMapFilter>
            <PatternLayout pattern="${default-pattern}"/>
        </Console>

        <Console name="CI" target="SYSTEM_OUT">
            <PatternLayout pattern="${pattern-with-trace-id}"/>
        </Console>

        <Console name="SimpleDefault" target="SYSTEM_OUT">
            <ContextMapFilter onMatch="ACCEPT" onMismatch="DENY">
                <KeyValuePair key="simple" value="true"/>
            </ContextMapFilter>
            <PatternLayout pattern="${simple-pattern}"/>
        </Console>

        <File name="LogFile" fileName="build.log">
            <PatternLayout pattern="${default-pattern}"/>
        </File>

        <File name="CILogFile" fileName="build.log">
            <Filters>
                <!-- Only log to file when ciMode is true -->
                <ThresholdFilter level="${log-level}" onMatch="ACCEPT" onMismatch="DENY"/>
            </Filters>
            <PatternLayout pattern="${default-pattern}"/>
        </File>

        <Rewrite name="SecretMaskRewrite">
            <MaskedDataInterceptor></MaskedDataInterceptor>
            <AppenderRef ref="ConsoleRouter"/>
            <AppenderRef ref="SimpleDefault"/>
            <AppenderRef ref="LogSplit"/>
            <AppenderRef ref="SimpleLogSplit"/>
            <AppenderRef ref="LogSplitCloud"/>
            <AppenderRef ref="SimpleLogSplitCloud"/>
            <AppenderRef ref="AppLogSplit"/>
            <AppenderRef ref="AppLogSplitCloud"/>
            <AppenderRef ref="LogFileRouter"/>
        </Rewrite>

        <Routing name="LogSplit">
            <ContextMapFilter onMatch="DENY" onMismatch="ACCEPT">
                <KeyValuePair key="simple" value="true"/>
            </ContextMapFilter>
            <Routes pattern="$${ctx:job_log_path}">
                <!-- Don't split logs if job_root isn't defined -->
                <Route key="$${ctx:job_log_path}">
                    <Null name="/dev/null"/>
                </Route>
                <Route>
                    <File name="${ctx:job_log_path}-local" fileName="${ctx:job_log_path}">
                        <PatternLayout pattern="${default-pattern}"/>
                    </File>
                </Route>
            </Routes>
            <IdlePurgePolicy timeToLive="${route-ttl}" timeUnit="minutes"/>
        </Routing>

        <Routing name="SimpleLogSplit">
            <ContextMapFilter onMatch="ACCEPT" onMismatch="DENY">
                <KeyValuePair key="simple" value="true"/>
            </ContextMapFilter>
            <Routes pattern="$${ctx:job_log_path}">
                <!-- Don't split logs if job_root isn't defined -->
                <Route key="$${ctx:job_log_path}">
                    <Null name="/dev/null"/>
                </Route>
                <Route>
                    <File name="${ctx:job_log_path}-local" fileName="${ctx:job_log_path}">
                        <PatternLayout pattern="${simple-pattern}"/>
                    </File>
                </Route>
            </Routes>
            <IdlePurgePolicy timeToLive="${route-ttl}" timeUnit="minutes"/>
        </Routing>

        <!--
            Separate routers are created for each cloud logger as
            1) a Route only accepts 1 appender
            2) Routes don't support routing log output to more than Route
        -->
        <Routing name="LogSplitCloud">
            <ContextMapFilter onMatch="DENY" onMismatch="ACCEPT">
                <KeyValuePair key="simple" value="true"/>
            </ContextMapFilter>
            <Routes pattern="$${ctx:cloud_job_log_path}">
                <!-- Don't split logs if job_root isn't defined -->
                <Route key="$${ctx:cloud_job_log_path}">
                    <Null name="/dev/null"/>
                </Route>
                <Route>
                    <Log4j2Appender name="${ctx:cloud_job_log_path}"
                      verbose="false"
                      stagingBufferAge="1"
                      azureBlobContainer="${azure-blob-container}"
                      azureBlobNamePrefix="job-logging${ctx:cloud_job_log_path}"
                      azureStorageConnectionString="${azure-storage-connection-string}"
                    >
                        <PatternLayout pattern="${default-pattern}"/>
                    </Log4j2Appender>
                </Route>
            </Routes>
            <IdlePurgePolicy timeToLive="${route-ttl}" timeUnit="minutes"/>
        </Routing>

        <Routing name="SimpleLogSplitCloud">
            <ContextMapFilter onMatch="ACCEPT" onMismatch="DENY">
                <KeyValuePair key="simple" value="true"/>
            </ContextMapFilter>
            <Routes pattern="$${ctx:cloud_job_log_path}">
                <!-- Don't split logs if job_root isn't defined -->
                <Route key="$${ctx:cloud_job_log_path}">
                    <Null name="/dev/null"/>
                </Route>
                <Route>
                    <Log4j2Appender name="${ctx:cloud_job_log_path}"
                      verbose="false"
                      stagingBufferAge="1"
                      azureBlobContainer="${azure-blob-container}"
                      azureBlobNamePrefix="job-logging${ctx:cloud_job_log_path}"
                      azureStorageConnectionString="${azure-storage-connection-string}"
                    >
                        <PatternLayout pattern="${simple-pattern}"/>
                    </Log4j2Appender>
                </Route>
            </Routes>
            <IdlePurgePolicy timeToLive="${route-ttl}" timeUnit="minutes"/>
        </Routing>

        <Routing name="AppLogSplit">
            <Routes pattern="$${ctx:workspace_app_root}">
                <!-- Don't split logs if workspace_app_log_root isn't defined -->
                <Route key="$${ctx:workspace_app_root}">
                    <Null name="/dev/null"/>
                </Route>
                <Route>
                    <RollingFile
                            name="${ctx:workspace_app_root}-local"
                            fileName="${ctx:workspace_app_root}/logs.log"
                            filePattern="${ctx:workspace_app_root}/logs.%i.log.gz"
                            ignoreExceptions="false">
                        <PatternLayout pattern="${default-pattern}"/>
                        <Policies>
                            <SizeBasedTriggeringPolicy size="100MB" />
                        </Policies>
                        <DefaultRolloverStrategy max="3" />
                    </RollingFile>
                </Route>
            </Routes>
            <IdlePurgePolicy timeToLive="${route-ttl}" timeUnit="minutes"/>
        </Routing>
        <Routing name="AppLogSplitCloud">
            <Routes pattern="$${ctx:cloud_workspace_app_root}">
                <!-- Don't split logs if workspace_app_log_root isn't defined -->
                <Route key="$${ctx:cloud_workspace_app_root}">
                    <Null name="/dev/null"/>
                </Route>
                <Route>
                    <Log4j2Appender name="app-logging/${ctx:cloud_workspace_app_root}/"
                      stagingBufferAge="1"
                      azureBlobContainer="${azure-blob-container}"
                      azureBlobNamePrefix="job-logging${ctx:cloud_job_log_path}"
                      azureStorageConnectionString="${azure-storage-connection-string}"
                    >
                        <PatternLayout pattern="${default-pattern}"/>
                    </Log4j2Appender>
                </Route>
            </Routes>
            <IdlePurgePolicy timeToLive="${route-ttl}" timeUnit="minutes"/>
        </Routing>

        <Routing name="ConsoleRouter">
            <Routes pattern="$${ci-mode}">
                <Route ref="CI" key="true"/>
                <Route ref="Default" key="false"/>
            </Routes>
        </Routing>

        <Routing name="LogFileRouter">
            <Routes pattern="$${ci-mode}">
                <Route ref="CILogFile" key="true"/>
                <Route ref="LogFile" key="false"/>
            </Routes>
        </Routing>
    </Appenders>

    <Loggers>
        <Root level="${log-level}">
            <!-- Use the rewrite policy to ensure that connector configuration secrets are masked when logged
            Any additional appender references should be added to the <Rewrite> section above in this document
            to ensure that any secrets logged via those appenders is masked.-->
            <AppenderRef ref="SecretMaskRewrite"/>
        </Root>

        <Logger name="org.eclipse.jetty" level="INFO" />
        <Logger name="com.github.dockerjava" level="INFO" />
        <Logger name="org.apache.hc" level="INFO" />
        <Logger name="org.jooq" level="INFO" />
        <logger name="org.jooq.Constants" level="OFF" />
        <Logger name="com.networknt.schema" level="INFO" />
        <Logger name="me.andrz.jackson" level="INFO" />
        <Logger name="com.leansoft.bigqueue" level="INFO" />
<!--        <logger name="io.micronaut.context.condition" level="TRACE" />-->
        <Logger name="io.netty" level="INFO" />
        <Logger name="io.grpc" level="INFO" />
        <Logger name="io.temporal" level="INFO" />
        <Logger name="org.apache" level="WARN" />
        <Logger name="httpclient" level="WARN" />
        <Logger name="com.zaxxer.hikari.pool.HikariPool" level="ERROR" />
        <Logger name="com.zaxxer.hikari.pool.PoolBase" level="ERROR" />
        <Logger name="com.zaxxer.hikari.HikariDataSource" level="ERROR" />
        <!--MySQL Debezium connector generates a log whenever it converts an invalid value to empty value.
        Ex: Invalid value '0000-00-00 00:00:00' stored in column 'column_name' of table 'table_name' converted to empty value
        If a database has tons of such values, the logs would be filled with such messages-->
        <Logger name="io.debezium.connector.mysql.MySqlValueConverters" level="OFF" />
        <!--MySQL Debezium connector generates a log whenever it comes across a DDL query to mention that it skipped it.
        If a database has tons of DDL queries, the logs would be filled with such messages-->
        <Logger name="io.debezium.relational.history" level="OFF" />

        <!--Uncomment the following to debug JOOQ generated SQL queries.-->
        <!--<Logger name="org.jooq.tools.LoggerListener" level="debug">-->
        <!--  <AppenderRef ref="Console"/>-->
        <!--</Logger>-->

    </Loggers>

</Configuration>
